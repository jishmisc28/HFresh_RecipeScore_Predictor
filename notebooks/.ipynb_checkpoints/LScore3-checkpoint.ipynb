{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from HFresh import cookUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\abhranshu\\\\Desktop\\\\Incubating\\\\Ab_ML_Notebooks\\\\LScore\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('../../ingredients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/abtpst/Kaggle-Whats-Cooking/tree/master/src/cook \n",
    "#%%capture\n",
    "%run cookUtil.py\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "#import cookValidate as cookVal\n",
    "import cookUtil \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''\n",
    "This method calculates and displays the best set of parameters for the sklearn Pipeline\n",
    "It also shows the accuracy of the classifier on the validation set\n",
    "Arguments:\n",
    "pipeline <=> sklearn Pipeline with TfidfVectorizer and LogisticRegression\n",
    "parameters <=> Parameters for initializing Pipeline components\n",
    "Returns:\n",
    "bestParameters <=> Best set of parameters for sklearn Pipeline after retraining with feedback data\n",
    "'''\n",
    "def getBestParameters(pipeline,parameters):\n",
    "\n",
    "    # Load training data\n",
    "    #traindf = pd.read_json('../../data/train.json')\n",
    "    data1 = pd.read_csv('./ingredients.csv')         \n",
    "    data2 = pd.read_csv('./recipe_ratings.csv')\n",
    "    \n",
    "    print (\"unique recipe codes in data1: \",data1.Recipe_code.nunique())\n",
    "    print (\"unique recipe codes in data2: \",data2.Recipe_code.nunique())\n",
    "\n",
    "    traindf = pd.merge(data1,data2,how='left',on='Recipe_code')    \n",
    "    \n",
    "    # Remove everything but alphabets and then Lemmatize. Also remove extra whitespace\n",
    "    #traindf['properIngredients'] = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in traindf['Ingredient']]       \n",
    "    # Create learning matrix\n",
    "    X, y = traindf['Ingredient'], traindf['score'].as_matrix()\n",
    "    # Split into Training and Validation Sets\n",
    "    Xtrain, Xvalidate, ytrain, yValidate = train_test_split(X, y, train_size=0.7)\n",
    "    # Initialize gridSearchClassifierCV Classifier with parameters\n",
    "    gridSearchClassifier = cookUtil.getGridSearchCv(pipeline, parameters)\n",
    "    # Fit/train the gridSearchClassifier on Training Set\n",
    "    gridSearchClassifier.fit(Xtrain, ytrain)\n",
    "    # Make predictions on validation set and calculate best set of parameters  \n",
    "    ##>  bestParameters,predictions=cookVal.validate(parameters, gridSearchClassifier, Xvalidate, yValidate)\n",
    "    # Initialize DataFrame for feedback loop\n",
    "    valdf = pd.DataFrame(index = Xvalidate.index.values)\n",
    "    # Add ingredients column\n",
    "    valdf=valdf.join(Xvalidate)\n",
    "    # Add correct cuisine\n",
    "    valdf[\"score\"] = yValidate\n",
    "    # Add predictions column\n",
    "    valdf[\"pred_cuisine\"] = predictions\n",
    "    # Add check column. This column would be false for incorrect predictions\n",
    "    valdf[\"check\"] = valdf.pred_cuisine==valdf.cuisine\n",
    "    # Store DataFrame for feedback\n",
    "    valdf.to_csv(\"./feedback.csv\")\n",
    "    # Create joint DataFrame to incorporate feedback data. As of now, this will only have the ingredients and cuisine columns from the training set\n",
    "    ultimateTraindf = pd.DataFrame(index = Xtrain.index.values)\n",
    "    ultimateTraindf=ultimateTraindf.join(Xtrain)\n",
    "    ultimateTraindf[\"score\"] = ytrain\n",
    "    # Calculate best set of parameters after retraining with feedback data. Make predictions on validation set \n",
    "    ##> bestParameters,predictions = cookVal.feedback(pipeline,parameters,ultimateTraindf)\n",
    "    # Return best set of parameters\n",
    "    return bestParameters\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    # Get Pipeline components\n",
    "    pipeline = cookUtil.getPipeline()\n",
    "    \n",
    "    # Get parameter options for Pipeline components\n",
    "    parameters = cookUtil.getParameters()\n",
    "    \n",
    "    # Get best set of parameters and evaluate validation set accuracy\n",
    "    bestParameters = getBestParameters(pipeline,parameters)\n",
    "\n",
    "    # Save best parameter set\n",
    "    res = open(\"./res.txt\", 'w')\n",
    "    res.write ('best parameters set:\\n')\n",
    "    \n",
    "    for paramName in sorted(parameters.keys()):\n",
    "        res.write('\\t %s: %r\\n' % (paramName, bestParameters[paramName]))\n",
    "    \n",
    "    #pickle.dump(bestParameters,open(\"../../picks/bestParams.pkl\",\"wb\"))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
